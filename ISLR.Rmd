---
title: "ISLR"
author: "GT"
date: "15 septembre 2016"
output: html_document
---

## Chapter 2 - Statistical Learning

### What is statistical learning ?

The **`Advertising`** data set consist of the **`Sales`** of a product in
200 different markets along with advertising budgets for the product
in each of those markets for three different media: **`TV`**, **`Radio`**
and **`Newspaper`**.

```{r echo=FALSE, fig.width=9}
Advertising = read.csv("Advertising.csv")
#names(Advertising)
lm.fit.TV=lm(Sales~TV,data=Advertising)
lm.fit.radio=lm(Sales~Radio,data=Advertising)
lm.fit.newspaper=lm(Sales~Newspaper,data=Advertising)
par(mfrow=c(1,3))
plot(Advertising$TV,Advertising$Sales,col="red",
     xlab="TV",ylab="Sales")
abline(lm.fit.TV,lwd=3,col="blue")
plot(Advertising$Radio,Advertising$Sales,col="red",
     xlab="Radio",ylab="Sales")
abline(lm.fit.radio,lwd=3,col="blue")
plot(Advertising$Newspaper,Advertising$Sales,col="red",
     xlab="Newspaper",ylab="Sales")
abline(lm.fit.newspaper,lwd=3,col="blue")
```

If we determine that there is an association between advertising and
sales, then we can instruct our client to adjust advertising budgets,
thereby indirectly increasing sales.

Our goal is to develop an accurate model that can be used to predict
sales on the basis of the three media budgets.

The advertising budgets are _input variables_ while **`Sales`** is an
_output variable_. Input variable are typically denoted using the
symbol $X$. $X_1$ might be the **`TV`** budget, $X_2$ the **`Radio`**
budget, and $X_3$ the **`Newspaper`** budget. The ouput variable is 
typicall denoted using the symbol $Y$.

+ The inputs are often called, _predictors_, _independent variables_,
_features_ or _variables_.
+ The output variable is often called _response_ or _dependent variable_

More generally, suppose that we observe a quantitative response $Y$
and $p$ different predictors, $X_1$,$X_2$,...,$X_p$. We assume that
there is some relationship between $Y$ and $X=(X_1,X_2,…,X_p)$ which
can be written in the very general form

$$Y=f(X)+\epsilon$$

$f$ is some fixed but unknown function of $X_1,…,X_p$, and $\epsilon$
is a random _error term_, which is independent of $X$ and has mean
zero. 
$f$ represents the _systematic_ information that $X$ provides about $Y$.

As another example, consider the plot of **`Income`** versus
**`Years of education`** for 30 individuals in the **`Income`**
data set. The plot suggests tthat one might be able to predict 
income using years of **`Education`**. However, the function $f$
that connects the input variable to the output variable is in
general unkwnow. Since **`Income`** is a simulated data set, $f$
is known and is shown by the blue curve. The vertical lines represent
the error terms $\epsilon$, overall, the errors have approximately
mean zero.

```{r echo=FALSE, fig.width=8}
Income = read.csv("Income1.csv")
par(mfrow=c(1,2))
plot(Income$Education,Income$Income,col="red", pch=20,
     xlab="Years of Education",ylab="Income")
nls.fit <- nls(Income~d+(a/(1+exp(-b*(Education-c)))),data=Income,
               start=list(a=90,b=0.5,c=15,d=20))
plot(Income$Education,Income$Income,col="red", pch=20,
     xlab="Years of Education",ylab="Income")
lines(Income$Education,fitted(nls.fit),lwd=2,col="blue")
npoints <- length(Income$Income)
for (k in 1: npoints) lines(c(Income$Education[k],Income$Education[k]),
                            c(Income$Income[k],fitted(nls.fit)[k]))
```

In essence, statistical learning refers to a set of approaches for 
estimating $f$.

#### Why Estimate $f$ ?

There are two main reasons that we may wish to estimate $f$:
_prediction_ and _inference_.

**Prediction**

In many situations, a set of inputs $X$ are readily available,
but the output $Y$ cannot be easily obtained. In this setting,
since the error term averages to zero, we can predict $Y$ using

$$\hat{Y}=\hat{f}(X)$$

where $\hat{f}$ represent our estimate for $f$, and $\hat{Y}$ 
represents the resulting prediction for $Y$. 
$\hat{f}$ is often treated as a _black box_, we're not typically
concerned with the exact form of $\hat{f}$, provided that it yields
accurate predictions for $Y$.

The accuracy of $\hat{Y}$ as a prediction for $Y$ depends on two
quantities, which we will call the _reducible error_ and the
_irreducible error_. In general, $\hat{f}$ will not be a perfect
estimate for $f$, and this inaccuracy will introduce some error.

This error is  _reducible_ because we can potentially improve the
accuracy of $\hat{f}$ by using the most appropriate statistical
learning technique to estimate $f$. Howeve, even if it were
possible to form a perfect estimate for $f$, $\hat{Y}=f(X)$, our
prediction would still have some error in it! This is because $Y$
is also a function of $\epsilon$, which cannot be predict using $X$.
This is known as the _irreducible error_ because no matter how well
we estimate $f$, we cannot reduce introduced by $\epsilon$.

**Inference**

We are often interested in urderstanding the way that $Y$ is affected
as $X_1,…,X_p$ change. We wish to estimate $f$, but our goal is
not necessarily to make predictions for $Y$. We want to understand
how $Y$ changes as a function of $X_1,…,X_p$. Now $\hat{f}$ cannot be
treated as a black box, because we need to know its exact form.
One may be interested in answering the following questions:

- _Which predictors are associated with the response?_ This is the 
  case when only a small fraction of the available predictors are
  substantially associated with $Y$.
- _What is the relationship between the response and each predictor?_
  Some predictors may have a positive relationship with $Y$, in the
  sense that increasing the predictor is associated with increasing
  values of $Y$. Other predictors may have the opposite 
  relationship.
- _Can the relationship between $Y$ and each predictor be adequately_
  _summarized using a linear equation, or is the relationship_
  _more complicated?_ Often the true relationship is more
  complicated, in which case a linear model may not provide an
  accurate representation of the relationship between the input
  and output variables.

For instance, consider a company that is interested in conducting a 
direct-marketing campaign. The goal is to identify individuals
who will respond positively to a mailing, based on observation of
demographic variables measured on each individual. The company is
not interested in obtaining a deep undestanding of the relationship
between each individual predictor and the response; instead, the
company simply wants an accurate model to predict the response
using the predictors. This is an example of modelling for _prediction_.

In contrast, consider the **`Advertising`** data. One may be 
interested  in answering questions such as:

- _Which media contribute to sales?_
- _Which media generate the biggest boost in sales?_
- _How much increase in sales is associated with a given increase_
  _in TV advertising?_

This situation falls into the _inference_ paradigm.

Finally, some modelling could be conducted both for prediction and
inference. For example, in a real estate setting, one may seek to
relate values of homes to inputs such as crime rate, zoning, distance
from a river, air quality, schools, income level of community, 
size of houses, and so forth. In this case one might be interested
in how the individual input variables affect the prices, 
_how much extra will a house be worth if it has a view of the river?_
This is an _inference problem_. Alternatively, one may simply be
interested in predicting the value of a home given it's characteristics:
_is this house under- or over-valued?_ This is a _prediction problem_.

_Linear models_ allow for relatively simple and interpretable
inference, but may not yield as accurate predictions as some other
approaches. In contrast, some of the highly non-linear approaches
can potetially provide quite accurate predictions for $Y$, but this
comes at the expense of a less interpretable model for which 
inference is more challenging.

#### How Do We Estimate $f$?

We will assume that we have observed a set of $n$ different data
points. These observations are called the _training data_ because
we will use these observations to train, or teach, our method
how to estimate $f$.

Let $x_{ij}$ represent the value of the $j$ th predictor, or input,
for observation $i$, where $i$ = 1,2,…,$n$ and $j$ = 1,2,…,$p$.
Correspondingly, let $y_i$ represent the response variable for
the $i$ th observation. Then our training data consist of
${(x_1,y_1),(x_2,y_2),…,(x_n,y_n)}$ where
$x_i=(x_{i1},x_{i2},…,x_{ip})^T$.

Our goal is to apply a statistical learning method to the training
data in order to estimate the unknown function $f$. In other words,
we want to find a function _\hat{f}_ such that $Y\approx\hat{f}(X)$
for any observation $(X,Y)$. Broadly speaking, most statistical
learning methods for this task can be characterized as either
_parametric_ or _non-parametric_.

**Parametric Methods**

Parametric methods involve two-step model-based approach.

1. First, we make an assumption about the functional form, or shape,
   of $f$. For example, one very simple assumption is that $f$ is 
   linear in $X$:
   
   $$f(X)=\beta_0+\beta_1X_1+\beta_2X_2+…+\beta_pX_p$$
   
   This is a _linear model_. Once we have assumed that $f$ is 
   linear, the problem of estimating $f$ is greatly simplified.
   Instead of having to estimate an entirely arbitrary 
   $p$-dimensional function $f(X)$, one only needs to estimate
   the $p+1$ coefficients $\beta_0$, $\beta_1$, …, $\beta_p$.
   
2. After a model has been selected, we need a procedure that uses
   the training data to _fit_ or _train_ the model. In the case
   of the linear model, we need to estimate the parameters 
   $\beta_0$, $\beta_1$, …, $\beta_p$. That is, we want to find
   values of these parameters such that
   
   $$Y\approx\beta_0+\beta_1X_1+\beta_2X_2+…+\beta_pX_p$$
   
   The most common approach to fitting the model is referred to
   as _(ordinary) least squares_.

The model-based approach just described is referred to as
_parametric_; it reduces the problem of estimating $f$ down
to one of estimating a set of parameters. The potential 
disadvantage of a parametric approach is that the model we
choose will usally not match the true unknown form of $f$.
If the chosen model is too far from the true $f$, then
our estimate will be poor.

We can try to address this problem by choosing _flexible_ 
models that can fit many different possible functional forms 
for $f$. But in general, fitting a more flexible model requires
estimating a greater number of parameters. These more
complex models can lead to a phenomenon known as _overfitting_
the data, which means they follow the errors, or _noise_ too
closely.

Figure shows an example of the parametric approach applied to
the **`Income`** data. We have a linear model of the form

$$\textbf{income}\approx\beta_0+\beta_1\times\textbf{education}$$

```{r echo=FALSE, fig.width=8}
lm.fit=lm(Income~Education,data=Income)
par(mfrow=c(1,2))
plot(Income$Education,Income$Income,col="red", pch=20,
     xlab="Years of Education",ylab="Income")
abline(lm.fit,lwd=2,col="blue")
for (k in 1: npoints) lines(c(Income$Education[k],Income$Education[k]),
                            c(Income$Income[k],fitted(lm.fit)[k]))
plot(Income$Education,Income$Income,col="red", pch=20,
     xlab="Years of Education",ylab="Income")
lines(Income$Education,fitted(nls.fit),lwd=2,col="blue")
for (k in 1: npoints) lines(c(Income$Education[k],Income$Education[k]),
                            c(Income$Income[k],fitted(nls.fit)[k]))
```

Since we have assumed a linear relationship between the response
and the predictor, the entire fitting problem reduces to estimating
$\beta_0$ and $\beta_1$, which we do using least squares linear
regression. In the figure we can see that the linear fit is not
quite right: the true $f$ has some curvature that is not 
captured in the linear fit. However, the linear fit still appears
to do a reasonable job of capturing the positive relationship
between **`years of education`** and **`income`**. It may be that
with such a small number of observations, this is the best we can do.

**Non-parametric Methods**

Non-parametric methods do not make explicit assumptions about
the functional form of $f$. Instead they seek an estimate of $f$
that gets as close to the data points as possible without being
too rough or wiggly. Such approaches can have a major advantage
over parametric approaches: by avoiding the assumption of a 
particular functional form for $f$, they have the potential
to accurately fit a wider range of possible shapes for $f$.
But non-parametric approaches do suffer from a major disadvantage:
since they do not reduce the problem of estimating $f$ to a
small number of parameters, a very large number of observations
is required in order to obtain an accurate estimate for $f$.

#### The Trade-Off Between Prediction Accuracy and Model Interpretability

Of the many methods, some are less flexible, or more restrictive,
in the sense that they can produce just a relatively small range
of shapes to estimate $f$. For example, linear regression is a
relatively inflexible approach, because it can only generate
linear functions.

One might reasonably ask the following question: _why would we_
_ever choose to use a more restrictive method instead of a_
_very flexible approach?_ If we are mainly interested in inference,
then restrictive models are much more interpretable.

For instance, when inference is the goal, there are clear 
advantages to using simple and relatively inflexible statistical
learning methods, as the linear model. This  may be a
good choice since it will be quite easy to understand the 
relationship between $Y$ and $X_1,X_2,…,X_p$. In contrast,
very flexible approaches, such as the splines, can lead to
such complicated estimates of $f$ that it is difficult to
understand how any individual predictor is associated with the
response.

In some settings, however, we are only interested in prediction,
and the interpretability of the predictive model is simply
not of interest. For instance, if we seek to develop an algorithm
to predict the price of a stock, our sole requirement for the
algorithm is that it predict accurately. In this setting, we
might expect thate it will be best to use the most flexible
model available.

Surprisingly, this is not always the case!

We will often obtain more accurate predictions using a less
flexible method. This phenomenon, which may seem counterintuitive
at first glance, has to do with the potential for overfitting
in highly flexible methods.

#### Supervised Versus Unsupervised Learning

Most statistical learning problems fall into one of two
categories: _supervised_ or _unsupervised_. For each observation
of the predictor measurement $x_i$, $i$=1,…,$n$ there is an
associated response measurement $y_i$. We wish to fit a model
that relates the response to the predictors, with the aim of
accurately predicting the response for future observations
(prediction) or better undertstanding the relationship between
the response and the predictors (inference). Many classical
statistical learning methods such as linear regression and
logistic regression operate in the supervised learning domain.

In contrast, unsupervised learning describes the somewhat
more challenging situation in which for every observation
$i$=1,…,$n$, we observe a vector of measurement $x_i$ but no 
associated response $y_i$. It is not possible to fit a
linear regression model, since there is no response variable
to predict. In this setting, we are in some sense working blind;
the situation is referred to as _unsupervised_ because we lack 
a response variable that can supervise our analysis. 

What sort of statistical analysis is possible ?

We can seek to understand the relationship between the 
variables or between the observations. One statistical
learning tool that we may use in this setting is 
_cluster analysis_, or clustering. 
The goal of cluster analysis is to ascertain, on the basis of
$x_1,…,x_n$, whether the observations fall into relatively
distinct groups. For example, in market segmentation study
we might observe multiple characteristics for potential 
customers, such a zip code, family income, and shopping
habits. We might believe that the customers fall into different
groups, such as big spenders versus low spenders. If the 
information about each customer's spending patterns were
available, then a supervised analysis would be possible.
However, this information is not available. In this setting,
we can try to cluster the customers on the basis of the 
variables measured, in order to identify distinct groups
of potential customers. Identify such groups can be of interest
because it might be that the groups differ with respect to
some property of interest, such as spending habits.

#### Regression Versus Classification Problems

Variables can be characterized as either _quantitative_ or
_qualitative_. Quantitative variables take on numerical values.
Examples include a person's age, height, or income. In contrast,
qualitative variables take on values on onk of $K$ different
_classes_, or categories. Examples include a person's gender
(male or female), the brand of product purchased (brand A, B or C),
or a cancer diagnosis (Acute Myelogenous Leukemia, Acute
Lymphoblastic Leukemia, or No Leukemia).

We tend to refer to problems with a quantitative response as
_regression_ problems, while those involving a qualitative 
response are often referred to as _classification_ problems.
However, the distinction is not always that crisp. Least squares
linear regression is used with a quantitative response, whereas
logistic regression is typically used with a qualitative 
response. As such it is often used as a classification method.
But since it estimates class probabilities, it can be thought
of as a regression method as well.

We tend to select statistical learning methods on the basis
of whether the response is quantitative or qualitative. 
However, whether the _predictors_ are qualitative or quantitative
is generally considered less important.

### Assessing Model Accuracy

Selecting the best approach can be one of the most challenging
parts of performing statistical learning in practice.

#### Measuring the Quality of Fit

In order to evaluate the performance of a statistical learning
method on a given data set, we need some way to measure how
well its predictions actually match the observed data. In the
regression setting, the most commonly-used measure is the
_mean squared error_ (MSE), given by

$$MSE=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{f}(x_i))^2$$

where $\hat{f}(x_i)$ is the prediction that $\hat{f}$ gives
for the $i$ th observation. The MSE will be small if the 
predicted response are very close to the true responses, and
will be large if for some of the observations, the predicted
and true responses differ substantially. 

The MSE is computed using the training data that was used
to fit the model, and so should more accurately be referred
to as the _training MSE_. But in general, we do not really care
how well the method works on the training data. Rather,
_we are interested in the accuracy of the predictions that_
_we obtain when we apply our method to previously unseen test data_.
Suppose that we are interested in developing an algorithm to
predict a stock's price based on previous stock returns. We can
train the method using stock returns from the past 6 months.
But we don't really care about how well it will predict tomorrow's
price or next month's price.

To state it more mathematically, suppose that we fit our statistical
learning method on our training observations
${(x_1,y_1),(x_2,y_2),…,(x_n,y_n)}$, and we obtain the estimate
$\hat{f}$. We can then compute $\hat{f}(x_1),\hat{f}(x_2),…,\hat{f}(x_n)$.
If these are approximately equal to $y_1,y_2,…,y_n$, then the
training MSE is small. However, we are really not interested in 
wheter $\hat{f}(x_i)\approx(y_i)$; instead, we want to know whether
$\hat{f}(x_0)$ is approximately equal to $y_0$, where $(x_0,y_0)$ is
a _previously unseen test observation not used to train the statistical_
_learning method_. We want to choose the method that gives the lowest
_test MSE_, as opposed to the lowest training MSE. In other words,
if we had a large number of test observations, we could compute

$$Ave(y_0-\hat{f}(x_0))^2$$

the average squared prediction error for these test observations 
$(x_0,y_0)$.

How can we go about trying to select a method that minimizes the test
MSE? 
In some settings, we may have a test data set available and we
can simply evaluate on the test observations, and select the learning
method for which the test MSE is smallest. But what if no test
observations are available? In that case, one might imagine simply
selecting a statistical learning method that minimizes the training MSE.
This seems like it might be a sensible approach, since the training
MSE and the test MSE appear to be closely related. Unfortunately, 
there is a fundamental problem with this strategy: there is no
guarantee that the method with the lowest training MSE will also
have the lowest test MSE. The problem is that many statistical 
methods specifically estimate coefficients so as to minimize the 
training set MSE. For these methods, the training set MSE can be 
quite small, but the test MSE is often much larger.

A fundamental property of statistical learning hold regardless of
the particular data set at hand and regardless of the statistical
method being used. As model flexibility increases, training MSE
will decrease, but the test MSE may not. When a given method
yields a small training MSE but a large test MSE, we are said 
to be _overfitting_ the data. This happens because our statistical
learning procedure is working too hard to find patterns that are just
caused by random chance rather than by true properties of the 
unknown function $f$. Overfitting refers specifically to the case
in which a less flexible model would have yielded a smaller test MSE.

#### The Bias-Variance Trade-Off

It is possible to show that the expected test MSE, for a given value
$x_0$, can always be decomposed into the sum of three fundamental
quantities: the _variance_ of $\hat{f}(x_0)$, the squared _bias_ of
$\hat{f}(x_0)$ and the variance of the error terms $\epsilon$. That is,

$$E(y_0-\hat{f}(x_0))^2=Var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2+Var(\epsilon)$$

Here the notation $E(y_0-\hat{f}(x_0))^2$ defines the _expected test MSE_,
and refers to the average test MSE that we would obtain if we
repeatedly estimated $f$ using a large number of training sets, and
tested each at $x_0$. The overall expected test MSE can be computed
by averaging $E(y_0-\hat{f}(x_0))^2$ over all possible values of $x_0$ 
in the test set. In order to minimize the expected test error, we need
to select a statistical learning method that simultaneously achieves
_low variance_ and _low bias_. Note that the variance is inherently
a nonegative quantity, and squared bias is also nonnegative. Hence, we
see that the expected test MSE can never lie below $Var(\epsilon)$, the
irreducible error.

What do we mean by the  _variance_ and _bias_ of a statistical learning
method ? 

_Variance_ refers to the amount by which $\hat{f}$ would
change if we estimated it using a different training data set. Since
the training data are used to fit the statistical learning method,
different training data sets will result in a different $\hat{f}$. But
ideally the estimate for $f$ should not vary too much between training
sets. In general, more flexible statistical methods have higher
variance, then small changes in the training data can result in large
changes in $\hat{f}$. In contrast, relative inflexible method has
low variance, because moving any single observation will likely cause
only a small shift in the estimate $\hat{f}$.

_Bias_ refers to the error that is introduced by approximating a
real-life problem, which may be extremely complicated, by a much
simpler model. For example, linear regression assumes that there is
a linear relationship between $Y$ and $X_1,X_2,…,X_p$. It is unlikely
that any real-life problem truly has such a simple linear relationship,
and so performing linear regression will undoubtedly result in some
bias in the estimate of $f$. If the relationship is non-linear, no
matter how many training observations we are given, it will not be
possible to produce an accurate estimate using linear regression, 
this results in high bias. However, the the true $f$ is very close
to linear, and so given enough data, it should be possible for linear
regression to produce an accurate estimate. Generally, more flexible
methods result in less bias.

As a general rule, as we use more flexible methods, the variance will
increase and the bias will decrease. The relative rate of change of
these two quantities determines wheter the test MSE increases or
decreases. As we increase the flexibility of a class of methods, the
bias tends to initially decrease faster than the variance increases.
Consequently, the expected test MSE declines. However, at some point
increasing flexibility has little impact on the bias but starts to
significantly increase the variance. When this happens the test MSE
increase.

The relationship between bias, variance, and test set MSE is referred
to as the _bias-variance trade-off_. This is because it is easy to
obtain a method with extremely low bias but high variance or a method
with very low variance but high bias. The challenge lies in finding
a method for which both the variance and the squared bias are low.
In a real-life situation in which $f$ is unobserved, it is generally
not possible to explicity compute the test MSE, bias or viariance for
a statistical learning method. Nevertheless, one should always keep
the bias-variance trade-off in mind.

#### The Classification Setting

Many of the concepts that we have encountered, such as the bias-variance
trade-off, transfer over to the classification setting with only some
modifications due to the fact that $y_i$ is no longer numerical.
Suppose that we seek to estimat $f$ on the basis of training observations
${(x_1,y_1),…,(x_n,y_n)}$, where now $y_1,…,y_n$ are qualitative. The
most common approach for quantifying the accuracy of our estimate
$\hat{f}$ is the training _error rate_, the proportion of mistakes that
are made if we apply our estimate $\hat{f}$ to the training observations:

$$\frac{1}{n}\sum_{i=1}^{n}I(y_i\neq\hat{y}_i)$$

Here $\hat{y}_i$ is the predicted class label for the $i$ th observation
using $\hat{f}$. And $I(y_i\neq\hat{y}_i)$ is an _indicator variable_ that
equals 1 if $y_i\neq\hat{y}_i$ and zero if $y_i=\hat{y}_i$. 
If $I(y_i\neq\hat{y}_i)=0$ then the $i$ th observation was classified
correctly by our classification method; otherwisi it was misclassified.
Hence computes the fraction of incorrect classifications. 
The equation is referred to as the _training error_ rate because it is
computed based on the data that was used to train our classifier. As in
the regression setting, we are most interested in the error rates that
result from applying our classifier to test observations that were
not used in training. The _test error_ rate associated with a set of
test observations of the form $(x_0,y_0)$ is given by

$$Ave(I(y_0\neq\hat{y}_0))$$

where $\hat{y}_0$ is the predicted class label that results from 
applying the classifier to the test observation with predictor $x_0$.
A _good_ classifier is one for which the test error is smallest.

**The Bayes Classifier**

It is possible to show that the test error rate is minimized, on
average, by a very simple classifier that _assigns each observation_
_to the most likely class, given its predictor values_. In other words,
we should simply assign a test observation with predictor vector $x_0$ to
the class $j$ for which

$$Pr(Y=j|X=x_0)$$

is largest. This is a _conditional probability_: it is the probability
that $Y=j$, given the observed predictor vector $x_0$. This very
simple classifier is called the _Bayes classifier_. In a two-class
problem where there are only two possible response values, 'class1' or
'class2', the Bayes classfier corresponds to predict class one
if $Pr(Y=1|X=x_0)>0.5$, and class two otherwise.

The Bayes classifier produces the lowest possible test error rate,
called the _Bayes error rate_. Since the Bayes classifier will
always choose the class for which $Pr(Y=j|X=x_0)$ is largest, the
error rate at $Y=x_0$ will be $1-max_jPr(Y=j|X=x_0)$. In general,
the overall Bayes error rate is given by

$$1-E(max_jPr(Y=j|X))$$

where the expectation averages the probability over all possible values
of X. The Bayes error rate is analogous to the irreducible error.

**K-Nearest Neighbors**


Load the `Auto` data set. This data set is part of the `ISLR` library.

```{r}
library(ISLR)
```


```{r}
dim(Auto)

# Check the variables names
names(Auto)
```

We can use the `plot()` function to produce _scatterplots_ of quantitative
variables.

```{r}
plot(Auto$cylinders, Auto$mpg)
```

The `cylinders` variable is stored as a numeric vector, so R has 
treated it as quantitative. However, since there are only a small
number possible values for cylinders, one may prefer to treat it
as a qualitative variable.

```{r}
# The as.factor() function converts quantitative variable into
# qualitative variable
Auto$cylinders = as.factor(Auto$cylinders)
```

If the variable plotted on the x-axis is categorical, then
_boxplots_ will be automatically be produced by the `plot()`
function.

```{r}
plot(Auto$cylinders, Auto$mpg, col="red", varwidth=T, xlab="cylinders",
     ylab="MPG")
```

The `hist()` function can be used to plot a _histogram_.

```{r}
hist(Auto$mpg, col=2, breaks=15)
```

The `pairs() function creates a _scatterplot matrix_.

```{r}
pairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)
```

The `summary()` function produces a numerical summary of each variable
in a particular data set

```{r}
summary(Auto)
```

## Chapter 3

### Simple Linear Regression

Here we load the `MASS` package, which is a very large collection of
data sets and functions. We also load the ISLR package, which includes
the data sets assoctiated with the book.

```{r}
library(MASS)
library(ISLR)
```

The `MASS` library contains the `Boston` data set, which records
`medv` (median house value) for 506 neighborhoods around Boston.
We will seek to predict `medv` using 13 predictors such as `rm` 
(average number of rooms per house), `age` (average age of houses), 
and `lstat` (percent of households with low socioeconomic status).

```{r}
names(Boston)
```

We will start by using the `lm()` function to fit a simple
linear regression model, with `medv` as the response and 
`lstat` as the predictor.

```{r}
lm.fit=lm(medv~lstat, data=Boston)
lm.fit
```

Some basic information about the model is output. For more
detailed information, we use `summary(lm.fit)`. This gives us
p-values and standard errors for the coefficients, as well as
the R-squared statistic and F-statistic for the model.

```{r}
summary(lm.fit)
```

We can use `names()` function in order to find out what other
pieces of information are stored in `lm.fit`. Although we can
extract these quantities by name, `lm.fit$coefficients`, it is
safer to use the extractor function like `coef()` to access them.

```{r}
names(lm.fit)
lm.fit$coefficients
coef(lm.fit)
```

In order to obtain a confidence interval for the coefficient
estimates, we can use the `confint()` command.

```{r}
confint(lm.fit)
```

The `predict()` function can be used to produce confidence
intervals and prediction intervals for the prediction of `medv`
for a given value of `lstat`.

```{r}
predict(lm.fit, data.frame(lstat=c(5,10,15)),
        interval="confidence")
predict(lm.fit, data.frame(lstat=c(5,10,15)),
        interval="prediction")
```

The 95% confidence interval associated with `lstat` value of
10 is (24.47, 25.63), and the 95% prediction interval is
(12.828, 37.28). As expected, the confidence and prediction
intervals are centered around the same point (25.05 for `medv`
when `lstat` equals 10), but the latter are substantially wider.

We will now plot `medv` and `lstat` along with the least squares
regression line using the `plot()` and `abline()` functions.

The `abline()` function can be used to draw any line, not just
the least squares regression line. To draw a line with intercept
`a` and slope `b`, we type `abline(a,b)`. The `lwd=3()` command 
causes the width of the regression line to be increased by a 
factor of 3; this works for the `plot()` and `lines()`  function
also. We can use the `pch` option to create different plotting
symbols.

```{r}
plot(Boston$lstat,Boston$medv)
abline(lm.fit, lwd=3,col="red")
```

Four diagnostic plots are automatically produced by
applying the `plot()` function directly to the output from `lm()`.
It is often convenient to view all four plots together. We can
achieve this by using the `par()` function, which tell `R` to
split the display screen into separete panels so that multiple 
plots can be viewed simultaneously.

```{r}
par(mfrow=c(2,2))
plot(lm.fit)
```

Alternatively, we can compute the residuals from a linear 
regression fit using the `residuals()` function. The function
`rstudent()` will return the studentized residuals, and we 
can use this function to plot the redisuals against the
fitted values.

```{r}
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
```

On the basis of the residuals plots, there is some evidence
of non-linearity. Leverage statistics can be computed for any
number of predictors using the `hatvalues()` function.

The `which.max()` function identifies the index of the largest
element of a vector. In this case, it tells us which obs has
the larsgest leverage

```{r}
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```

### Multiple Linear Regression

In order to fit a multiple linear regression model using 
least squres, we again use the `lm()` function.

```{r}
lm.fit=lm(medv~lstat+age, data=Boston)
summary(lm.fit)
```

The `Boston` data set contains 13 variables, and so it would be
cumbersome to have to tye all of these in order to perform a
regression using all of the predictors. Instead, we can use the
following short-hand:

```{r}
lm.fit=lm(medv~., data=Boston)
summary(lm.fit)
```

We can access the individual components of a summary object by
name. 

```{r}
summary(lm.fit)$r.sq # give us the R-squared
summary(lm.fit)$sigma # give us the RSE
```

The `vif()` function, part of the `car` package, can be used to
compute variance inflation factors. Most VIF's are low to moderate
for this data.

```{r}
library(car)
vif(lm.fit)
```

What if we would like to perform a regression using all of the
variable but one?

```{r}
lm.fit1=lm(medv~.-age,data=Boston)
summary(lm.fit1)
```

### Interaction Terms

It is easy to include interaction terms in a linear model
using the `lm()` function. The syntax `lstat:black` tells `R`
to include an interaction term between `lstat` and `black`.
The syntax `lstat*age` simultaneously includes `lstat`, `age`,
and the interaction term `lstat`x`age` as predictors; it is
a shorthand for `lstat+age+lstat:age`.

```{r}
summary(lm(medv~lstat*age, data=Boston))
```

### Non-linear Transformations of the Predictors

The `lm()` function can also accomodate non-linear transformations
of the predictors. For instance, given a predictor X, we can
create a predictor X-squared using `I(X^2)`. The function `I()`
is needed since the `^` has a special meaning in a formula;
wrapping as we do allows the standard usage in `R`, which
is to raise `X` to the power `2`. We now perform a regression
of `medv` onto `lstat` and `lstat`-squared.

```{r}
lm.fit2=lm(medv~lstat+I(lstat^2),data=Boston)
summary(lm.fit2)
```

The near-zero p-value associated with the quadratic term suggests
that it leads to an improved model. We use the `anova()` function
to further quantify the extent to which the quadratitc fit is
superior to the linear fit.

```{r}
lm.fit=lm(medv~lstat, data=Boston)
anova(lm.fit,lm.fit2)
```

Here Model 1 represents the linear submodel containing only one
predictor, `lstat`, while Model 2 corresponds to the larger
quadratic model that has two predictors, `lstat` and `last`-squared.
The `anova()` function performs a hypothesis test comparing the 
two models. The null hypothesis is that the two models fit the
data equally well, and the alternative hypothesis is that the
full model is superior. Here the F-statistic is 135 and the
associated p-value is virtually zero. This provides very clear
evidence that the model containing the predictors `lstat` and
`last'-squared is far superior to the model that only contains
the predictor `lstat`. This is not surprising, since earlier we
saw evidence for non-linearity in the relationship between
`medv` and `lstat`. If we type

```{r}
par(mfrow=c(2,2))
plot(lm.fit2)
```

then we see that when the `lstat`-squared term is included in
the model, there is little discernible pattern in the residuals.

In order to create a cubic fit, we can include a predictor of the
form `I(X^3)`. However, this approach can start to get cumbersome
for highr-order polynomials. A better approach involves using
the `poly()` function to create the polynomial within `lm()`.
For example the following command produces a fifth-order
polynomial fit.

```{r}
lm.fit5=lm(medv~poly(lstat,5),data=Boston)
summary(lm.fit5)
```

This suggests that including additional polynomial terms, up to
fifth order, leads to an improvement in the model fit. However,
further investigation of the data reveals that no polynomials
terms beyond fifth order have significant p-values in regression
fit.

Of course, we are in no way restricted to using polynomial 
transformations of the predictors. Here we try a log transformation

```{r}
summary(lm(medv~log(rm),data=Boston))
```

### Qualitative Predictors 

We will now examine the `Carseats` data, which is part of the
`ISLR` library. We will attempt to predict `Sales` in 400
locations based on a number of predictors.

```{r}
names(Carseats)
```

The `Carseats` data includes qualitative predictors such as
`Shelveloc`, an indicator of the quality of the shelving location,
the space within a store in which the car seat is displayed, at
each location. The predictor `Shelveloc` takes on three possible
values, _Bad_, _Medium_ and _Good_.

Given a qualitative variable such as `Shelveloc`, `R` generates
dummy variables automatically. Below we fit a multiple regression
 model that includes some interaction termis.
 
 
```{r}
lm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)
```

The `contrast()` function returns the coding that `R` uses
for the dummy variables.

```{r}
contrasts(Carseats$ShelveLoc)
```

`R` has created a `ShelveLocGood` dummy variable that takes on a
value of 1 if the shelving location is good, and 0 otherwise.
It has also created a `ShelveLocMedium` dummy variable that 
equals 1 if the shelving location is medium, and 0 otherwise.
A bad shelving location corresponds to a zero for each of the
two dummy variables. The fact that the coefficient for 
`ShelveLocGood`  in the regression output is positive indicates
that a good shelvig location is associated with high sales.
And `ShelveLocMedium` has a smaller positive coefficient,
indicating that a medium shelving location leads to higher
sales than a bad shelving location but lower sales than a good
shelving location.

### Writing Functions

As we have seen, `R` comes with useful functions, and still more
functions are available by way of `R` libraries. However, we
will often be interested in performing an operation for which
no function is available. In this setting, we may want to write
our own function. For instance, below we provide a simple function
that reads in the `ISLR` and `MASS` libraries, called 
`LoadLibraries()`. 
The `{` symbol informs `R` that multiple commands are about to
be input.

```{r}
LoadLibraries=function(){
  library(ISLR)
  library(MASS)
  print("The libraries have been loaded.")
}
LoadLibraries # tell us what is in the function
LoadLibraries() # we call the  function
```

