---
title: "ISLR"
author: "GT"
date: "15 septembre 2016"
output: html_document
---

## Chapter 2 - Statistical Learning

### What is statistical learning ?

The **`Advertising`** data set consist of the **`Sales`** of a product in
200 different markets along with advertising budgets for the product
in each of those markets for three different media: **`TV`**, **`Radio`**
and **`Newspaper`**.

```{r echo=FALSE, fig.width=9}
Advertising = read.csv("Advertising.csv")
#names(Advertising)
lm.fit.TV=lm(Sales~TV,data=Advertising)
lm.fit.radio=lm(Sales~Radio,data=Advertising)
lm.fit.newspaper=lm(Sales~Newspaper,data=Advertising)
par(mfrow=c(1,3))
plot(Advertising$TV,Advertising$Sales,col="red",
     xlab="TV",ylab="Sales")
abline(lm.fit.TV,lwd=3,col="blue")
plot(Advertising$Radio,Advertising$Sales,col="red",
     xlab="Radio",ylab="Sales")
abline(lm.fit.radio,lwd=3,col="blue")
plot(Advertising$Newspaper,Advertising$Sales,col="red",
     xlab="Newspaper",ylab="Sales")
abline(lm.fit.newspaper,lwd=3,col="blue")
```

If we determine that there is an association between advertising and
sales, then we can instruct our client to adjust advertising budgets,
thereby indirectly increasing sales.

Our goal is to develop an accurate model that can be used to predict
sales on the basis of the three media budgets.

The advertising budgets are _input variables_ while **`Sales`** is an
_output variable_. Input variable are typically denoted using the
symbol $X$. $X_1$ might be the **`TV`** budget, $X_2$ the **`Radio`**
budget, and $X_3$ the **`Newspaper`** budget. The ouput variable is 
typicall denoted using the symbol $Y$.

+ The inputs are often called, _predictors_, _independent variables_,
_features_ or _variables_.
+ The output variable is often called _response_ or _dependent variable_

More generally, suppose that we observe a quantitative response $Y$
and $p$ different predictors, $X_1$,$X_2$,...,$X_p$. We assume that
there is some relationship between $Y$ and $X=(X_1,X_2,…,X_p)$ which
can be written in the very general form

$$Y=f(X)+\epsilon$$

$f$ is some fixed but unknown function of $X_1,…,X_p$, and $\epsilon$
is a random _error term_, which is independent of $X$ and has mean
zero. 
$f$ represents the _systematic_ information that $X$ provides about $Y$.

As another example, consider the plot of **`Income`** versus
**`Years of education`** for 30 individuals in the **`Income`**
data set. The plot suggests tthat one might be able to predict 
income using years of **`Education`**. However, the function $f$
that connects the input variable to the output variable is in
general unkwnow. Since **`Income`** is a simulated data set, $f$
is known and is shown by the blue curve. The vertical lines represent
the error terms $\epsilon$, overall, the errors have approximately
mean zero.

```{r echo=FALSE, fig.width=8}
Income = read.csv("Income1.csv")
par(mfrow=c(1,2))
plot(Income$Education,Income$Income,col="red", pch=20,
     xlab="Years of Education",ylab="Income")
nls.fit <- nls(Income~d+(a/(1+exp(-b*(Education-c)))),data=Income,
               start=list(a=90,b=0.5,c=15,d=20))
plot(Income$Education,Income$Income,col="red", pch=20,
     xlab="Years of Education",ylab="Income")
lines(Income$Education,fitted(nls.fit),lwd=2,col="blue")
npoints <- length(Income$Income)
for (k in 1: npoints) lines(c(Income$Education[k],Income$Education[k]),
                            c(Income$Income[k],fitted(nls.fit)[k]))
```

In essence, statistical learning refers to a set of approaches for 
estimating $f$.

#### Why Estimate $f$ ?

There are two main reasons that we may wish to estimate $f$:
_prediction_ and _inference_.

**Prediction**

In many situations, a set of inputs $X$ are readily available,
but the output $Y$ cannot be easily obtained. In this setting,
since the error term averages to zero, we can predict $Y$ using

$$\hat{Y}=\hat{f}(X)$$

where $\hat{f}$ represent our estimate for $f$, and $\hat{Y}$ 
represents the resulting prediction for $Y$. 
$\hat{f}$ is often treated as a _black box_, we're not typically
concerned with the exact form of $\hat{f}$, provided that it yields
accurate predictions for $Y$.

The accuracy of $\hat{Y}$ as a prediction for $Y$ depends on two
quantities, which we will call the _reducible error_ and the
_irreducible error_. In general, $\hat{f}$ will not be a perfect
estimate for $f$, and this inaccuracy will introduce some error.

This error is  _reducible_ because we can potentially improve the
accuracy of $\hat{f}$ by using the most appropriate statistical
learning technique to estimate $f$. Howeve, even if it were
possible to form a perfect estimate for $f$, $\hat{Y}=f(X)$, our
prediction would still have some error in it! This is because $Y$
is also a function of $\epsilon$, which cannot be predict using $X$.
This is known as the _irreducible error_ because no matter how well
we estimate $f$, we cannot reduce introduced by $\epsilon$.

**Inference**

We are often interested in urderstanding the way that $Y$ is affected
as $X_1,…,X_p$ change. We wish to estimate $f$, but our goal is
not necessarily to make predictions for $Y$. We want to understand
how $Y$ changes as a function of $X_1,…,X_p$. Now $\hat{f}$ cannot be
treated as a black box, because we need to know its exact form.
One may be interested in answering the following questions:

- _Which predictors are associated with the response?_ This is the 
  case when only a small fraction of the available predictors are
  substantially associated with $Y$.
- _What is the relationship between the response and each predictor?_
  Some predictors may have a positive relationship with $Y$, in the
  sense that increasing the predictor is associated with increasing
  values of $Y$. Other predictors may have the opposite 
  relationship.
- _Can the relationship between $Y$ and each predictor be adequately_
  _summarized using a linear equation, or is the relationship_
  _more complicated?_ Often the true relationship is more
  complicated, in which case a linear model may not provide an
  accurate representation of the relationship between the input
  and output variables.

For instance, consider a company that is interested in conducting a 
direct-marketing campaign. The goal is to identify individuals
who will respond positively to a mailing, based on observation of
demographic variables measured on each individual. The company is
not interested in obtaining a deep undestanding of the relationship
between each individual predictor and the response; instead, the
company simply wants an accurate model to predict the response
using the predictors. This is an example of modelling for _prediction_.

In contrast, consider the **`Advertising`** data. One may be 
interested  in answering questions such as:

- _Which media contribute to sales?_
- _Which media generate the biggest boost in sales?_
- _How much increase in sales is associated with a given increase_
  _in TV advertising?_

This situation falls into the _inference_ paradigm.

Finally, some modelling could be conducted both for prediction and
inference. For example, in a real estate setting, one may seek to
relate values of homes to inputs such as crime rate, zoning, distance
from a river, air quality, schools, income level of community, 
size of houses, and so forth. In this case one might be interested
in how the individual input variables affect the prices, 
_how much extra will a house be worth if it has a view of the river?_
This is an _inference problem_. Alternatively, one may simply be
interested in predicting the value of a home given it's characteristics:
_is this house under- or over-valued?_ This is a _prediction problem_.

_Linear models_ allow for relatively simple and interpretable
inference, but may not yield as accurate predictions as some other
approaches. In contrast, some of the highly non-linear approaches
can potetially provide quite accurate predictions for $Y$, but this
comes at the expense of a less interpretable model for which 
inference is more challenging.

#### How Do We Estimate $f$?


Load the `Auto` data set. This data set is part of the `ISLR` library.

```{r}
library(ISLR)
```


```{r}
dim(Auto)

# Check the variables names
names(Auto)
```

We can use the `plot()` function to produce _scatterplots_ of quantitative
variables.

```{r}
plot(Auto$cylinders, Auto$mpg)
```

The `cylinders` variable is stored as a numeric vector, so R has 
treated it as quantitative. However, since there are only a small
number possible values for cylinders, one may prefer to treat it
as a qualitative variable.

```{r}
# The as.factor() function converts quantitative variable into
# qualitative variable
Auto$cylinders = as.factor(Auto$cylinders)
```

If the variable plotted on the x-axis is categorical, then
_boxplots_ will be automatically be produced by the `plot()`
function.

```{r}
plot(Auto$cylinders, Auto$mpg, col="red", varwidth=T, xlab="cylinders",
     ylab="MPG")
```

The `hist()` function can be used to plot a _histogram_.

```{r}
hist(Auto$mpg, col=2, breaks=15)
```

The `pairs() function creates a _scatterplot matrix_.

```{r}
pairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)
```

The `summary()` function produces a numerical summary of each variable
in a particular data set

```{r}
summary(Auto)
```

## Chapter 3

### Simple Linear Regression

Here we load the `MASS` package, which is a very large collection of
data sets and functions. We also load the ISLR package, which includes
the data sets assoctiated with the book.

```{r}
library(MASS)
library(ISLR)
```

The `MASS` library contains the `Boston` data set, which records
`medv` (median house value) for 506 neighborhoods around Boston.
We will seek to predict `medv` using 13 predictors such as `rm` 
(average number of rooms per house), `age` (average age of houses), 
and `lstat` (percent of households with low socioeconomic status).

```{r}
names(Boston)
```

We will start by using the `lm()` function to fit a simple
linear regression model, with `medv` as the response and 
`lstat` as the predictor.

```{r}
lm.fit=lm(medv~lstat, data=Boston)
lm.fit
```

Some basic information about the model is output. For more
detailed information, we use `summary(lm.fit)`. This gives us
p-values and standard errors for the coefficients, as well as
the R-squared statistic and F-statistic for the model.

```{r}
summary(lm.fit)
```

We can use `names()` function in order to find out what other
pieces of information are stored in `lm.fit`. Although we can
extract these quantities by name, `lm.fit$coefficients`, it is
safer to use the extractor function like `coef()` to access them.

```{r}
names(lm.fit)
lm.fit$coefficients
coef(lm.fit)
```

In order to obtain a confidence interval for the coefficient
estimates, we can use the `confint()` command.

```{r}
confint(lm.fit)
```

The `predict()` function can be used to produce confidence
intervals and prediction intervals for the prediction of `medv`
for a given value of `lstat`.

```{r}
predict(lm.fit, data.frame(lstat=c(5,10,15)),
        interval="confidence")
predict(lm.fit, data.frame(lstat=c(5,10,15)),
        interval="prediction")
```

The 95% confidence interval associated with `lstat` value of
10 is (24.47, 25.63), and the 95% prediction interval is
(12.828, 37.28). As expected, the confidence and prediction
intervals are centered around the same point (25.05 for `medv`
when `lstat` equals 10), but the latter are substantially wider.

We will now plot `medv` and `lstat` along with the least squares
regression line using the `plot()` and `abline()` functions.

The `abline()` function can be used to draw any line, not just
the least squares regression line. To draw a line with intercept
`a` and slope `b`, we type `abline(a,b)`. The `lwd=3()` command 
causes the width of the regression line to be increased by a 
factor of 3; this works for the `plot()` and `lines()`  function
also. We can use the `pch` option to create different plotting
symbols.

```{r}
plot(Boston$lstat,Boston$medv)
abline(lm.fit, lwd=3,col="red")
```

Four diagnostic plots are automatically produced by
applying the `plot()` function directly to the output from `lm()`.
It is often convenient to view all four plots together. We can
achieve this by using the `par()` function, which tell `R` to
split the display screen into separete panels so that multiple 
plots can be viewed simultaneously.

```{r}
par(mfrow=c(2,2))
plot(lm.fit)
```

Alternatively, we can compute the residuals from a linear 
regression fit using the `residuals()` function. The function
`rstudent()` will return the studentized residuals, and we 
can use this function to plot the redisuals against the
fitted values.

```{r}
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
```

On the basis of the residuals plots, there is some evidence
of non-linearity. Leverage statistics can be computed for any
number of predictors using the `hatvalues()` function.

The `which.max()` function identifies the index of the largest
element of a vector. In this case, it tells us which obs has
the larsgest leverage

```{r}
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```

### Multiple Linear Regression

In order to fit a multiple linear regression model using 
least squres, we again use the `lm()` function.

```{r}
lm.fit=lm(medv~lstat+age, data=Boston)
summary(lm.fit)
```

The `Boston` data set contains 13 variables, and so it would be
cumbersome to have to tye all of these in order to perform a
regression using all of the predictors. Instead, we can use the
following short-hand:

```{r}
lm.fit=lm(medv~., data=Boston)
summary(lm.fit)
```

We can access the individual components of a summary object by
name. 

```{r}
summary(lm.fit)$r.sq # give us the R-squared
summary(lm.fit)$sigma # give us the RSE
```

The `vif()` function, part of the `car` package, can be used to
compute variance inflation factors. Most VIF's are low to moderate
for this data.

```{r}
library(car)
vif(lm.fit)
```

What if we would like to perform a regression using all of the
variable but one?

```{r}
lm.fit1=lm(medv~.-age,data=Boston)
summary(lm.fit1)
```

### Interaction Terms

It is easy to include interaction terms in a linear model
using the `lm()` function. The syntax `lstat:black` tells `R`
to include an interaction term between `lstat` and `black`.
The syntax `lstat*age` simultaneously includes `lstat`, `age`,
and the interaction term `lstat`x`age` as predictors; it is
a shorthand for `lstat+age+lstat:age`.

```{r}
summary(lm(medv~lstat*age, data=Boston))
```

### Non-linear Transformations of the Predictors

The `lm()` function can also accomodate non-linear transformations
of the predictors. For instance, given a predictor X, we can
create a predictor X-squared using `I(X^2)`. The function `I()`
is needed since the `^` has a special meaning in a formula;
wrapping as we do allows the standard usage in `R`, which
is to raise `X` to the power `2`. We now perform a regression
of `medv` onto `lstat` and `lstat`-squared.

```{r}
lm.fit2=lm(medv~lstat+I(lstat^2),data=Boston)
summary(lm.fit2)
```

The near-zero p-value associated with the quadratic term suggests
that it leads to an improved model. We use the `anova()` function
to further quantify the extent to which the quadratitc fit is
superior to the linear fit.

```{r}
lm.fit=lm(medv~lstat, data=Boston)
anova(lm.fit,lm.fit2)
```

Here Model 1 represents the linear submodel containing only one
predictor, `lstat`, while Model 2 corresponds to the larger
quadratic model that has two predictors, `lstat` and `last`-squared.
The `anova()` function performs a hypothesis test comparing the 
two models. The null hypothesis is that the two models fit the
data equally well, and the alternative hypothesis is that the
full model is superior. Here the F-statistic is 135 and the
associated p-value is virtually zero. This provides very clear
evidence that the model containing the predictors `lstat` and
`last'-squared is far superior to the model that only contains
the predictor `lstat`. This is not surprising, since earlier we
saw evidence for non-linearity in the relationship between
`medv` and `lstat`. If we type

```{r}
par(mfrow=c(2,2))
plot(lm.fit2)
```

then we see that when the `lstat`-squared term is included in
the model, there is little discernible pattern in the residuals.

In order to create a cubic fit, we can include a predictor of the
form `I(X^3)`. However, this approach can start to get cumbersome
for highr-order polynomials. A better approach involves using
the `poly()` function to create the polynomial within `lm()`.
For example the following command produces a fifth-order
polynomial fit.

```{r}
lm.fit5=lm(medv~poly(lstat,5),data=Boston)
summary(lm.fit5)
```

This suggests that including additional polynomial terms, up to
fifth order, leads to an improvement in the model fit. However,
further investigation of the data reveals that no polynomials
terms beyond fifth order have significant p-values in regression
fit.

Of course, we are in no way restricted to using polynomial 
transformations of the predictors. Here we try a log transformation

```{r}
summary(lm(medv~log(rm),data=Boston))
```

### Qualitative Predictors 

We will now examine the `Carseats` data, which is part of the
`ISLR` library. We will attempt to predict `Sales` in 400
locations based on a number of predictors.

```{r}
names(Carseats)
```

The `Carseats` data includes qualitative predictors such as
`Shelveloc`, an indicator of the quality of the shelving location,
the space within a store in which the car seat is displayed, at
each location. The predictor `Shelveloc` takes on three possible
values, _Bad_, _Medium_ and _Good_.

Given a qualitative variable such as `Shelveloc`, `R` generates
dummy variables automatically. Below we fit a multiple regression
 model that includes some interaction termis.
 
 
```{r}
lm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)
```

The `contrast()` function returns the coding that `R` uses
for the dummy variables.

```{r}
contrasts(Carseats$ShelveLoc)
```

`R` has created a `ShelveLocGood` dummy variable that takes on a
value of 1 if the shelving location is good, and 0 otherwise.
It has also created a `ShelveLocMedium` dummy variable that 
equals 1 if the shelving location is medium, and 0 otherwise.
A bad shelving location corresponds to a zero for each of the
two dummy variables. The fact that the coefficient for 
`ShelveLocGood`  in the regression output is positive indicates
that a good shelvig location is associated with high sales.
And `ShelveLocMedium` has a smaller positive coefficient,
indicating that a medium shelving location leads to higher
sales than a bad shelving location but lower sales than a good
shelving location.

### Writing Functions

As we have seen, `R` comes with useful functions, and still more
functions are available by way of `R` libraries. However, we
will often be interested in performing an operation for which
no function is available. In this setting, we may want to write
our own function. For instance, below we provide a simple function
that reads in the `ISLR` and `MASS` libraries, called 
`LoadLibraries()`. 
The `{` symbol informs `R` that multiple commands are about to
be input.

```{r}
LoadLibraries=function(){
  library(ISLR)
  library(MASS)
  print("The libraries have been loaded.")
}
LoadLibraries # tell us what is in the function
LoadLibraries() # we call the  function
```

