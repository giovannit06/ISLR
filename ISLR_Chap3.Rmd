---
title: "ISLR - Chap3"
author: "GT"
date: "October 6, 2016"
output: html_document
---


## Chapter 3 - Linear Regression

Linear regression is a useful tool for predicting a quantitative
response. Recall the *`Advertising`* data that contains *`sales`*.
for a particular product as a function of advertising budget for 
*`TV`*, *`radio`*, *`newspaper`* media. Suppose that as statistical
consultant we are asked to suggest, on the basis of this data,
a marketing plan for next year that will result in
high product sales. What information would be useful in order
to provide such a reccomendation? Here are a few important questions
that we might seek to address:

1. Is there a relationship between advertising budget and sales ?
2. How strong is the relationship between advertising budget and
   sales ?
3. Which media contribute to sales?
4. How accurately can we estimate the effect of each medium on sales ?
5. How accurately can we predict future sales ?
6. Is the relationship linear ?
7. Is there synergy among the advertising media? 


### Simple Linear Regression

_Simple linear regression_ lives up to its name it is a very 
straightforward approach for predicting a quantitative response
$Y$ on the basis of a single predictor variable $X$. It assumes 
that there is approximately a linear relationship between $X$ and
$Y$.

$$Y\approx\beta_0+\beta_1X$$

You might read "$\approx$" as "_is approximately modeled as_".
We will sometimes describe the equation by saying that we are
_regressing Y on X_. For example, $X$ may represent *`TV`*
advertising and $Y$ may represent *`sales`*. Then we can regress
*`sales`* onto *`TV`* by fitting the model.

$$\textbf{sales}\approx\beta_0+\beta_1\times\textbf{TV}$$

$\beta_0$ and $\beta_1$ are two unknown constants that represent
the _intercept_ and _slope_ terms in the linear model. Together,
they are known as the model _coefficients_ or _parameters_.
Once we have used our training data to produce estimates $\hat{\beta}_0$
and $\hat{\beta}_1$ for the model coefficients, we can predict
future sales on the basis of a particular value of TV advertising
by computing

$$\hat{y}=\hat{\beta}_0+\hat{\beta}_1x$$

where $\hat{y}$ indicates a prediction of $Y$ of the basis of $X=x$.
Here we use a _hat_ symbol, ^, to denote the estimated value for
un unknown parameter or coefficient, or to denote the predicted value
of the response.

#### Estimating the Coefficients

In practice, $\beta_0$ and $\beta_1$ are unknown. So before
we can make predictions, we must use data to estimate the coefficients.
Let

$$(x_1,y_1),(x_2,y_2),…,(x_n,y_n)$$

represent $n$ observation pairs, each of which consists of a 
measurement of $X$ and a measurement of $Y$. In the *`Advertising`*
example, this data set consist of the TV advertising budget and 
product sales in $n=200$ different markets. Our goal is to obtain 
coefficient estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ such that
the linear model fits the available data well so that
$y_i\approx\hat{\beta}_0+\hat{\beta}_1x_i$ for $i=1,…,n$. In other
words, we want to find a intercept $\hat{\beta}_0$ and a slope
$\hat{\beta}_1$ such that the resulting line is as close as 
possible to the $n=200$ data points.

There are a number of ways of measuring _closeness_. However,
by far the most common approach involves minimizing the _last squares_
criterion.

```{r echo=FALSE, fig.width=9}
Advertising = read.csv("Advertising.csv")
#names(Advertising)
lm.fit.TV=lm(Sales~TV,data=Advertising)
plot(Advertising$TV,Advertising$Sales,col="red",
     xlab="TV",ylab="Sales",pch=20)
abline(lm.fit.TV,lwd=2,col="blue")
npoints <- length(Advertising$Sales)
for (k in 1: npoints) lines(c(Advertising$TV[k],Advertising$TV[k]),
                            c(Advertising$Sales[k],fitted(lm.fit.TV)[k]),
                            col="grey")
```

Let $\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1x_1$ be the prediction
for $Y$ based on the $i$ th value of $X$. Then $e_i=y_i-\hat{y}_i$
represent the $i$ th _residual_, this is the difference between
the $i$ th observed response value and the $i$ th response value
that is predicted by our linear model. We define the
_residual sum of squares_ (RSS) as

$$RSS=e_1^2+e_2^2+…+e_n^2$$

or equivalently as 

$$RSS = (y_1-\hat{\beta}_0-\hat{\beta}_1x_1)^2+(y_2-\hat{\beta}_0-\hat{\beta}_1x_2)^2+…+(y_n-\hat{\beta}_0-\hat{\beta}_1x_n)^2$$

The least squares approach chooses $\hat{\beta}_0$ and $\hat{\beta}_1$
to minimize the RSS. Using some calculus, one can show that minimizers
are

$$\hat{\beta}_1=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$

$$\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}$$

where $\bar{y}\equiv\frac{1}{n}\sum_{i=1}^{n}y_i$ and
$\bar{x}\equiv\frac{1}{n}\sum_{i=1}^{n}x_i$ are the sample maeans.
In other words the equation defines the _least squares coefficient_
_estimates_ for simple linear regression.

In the previously figure, the simple linear regression fit for
the *`Advertising`* data, where $\hat{\beta}_0$=7.03 and 
$\hat{\beta}_1$=0.0475. In other words, according to this approximation,
an additional $1000 spent on TV advertising is associated with selling
approximately 47.5 additional units of product.

#### Assessing the Accuracy of thfe Coefficient Estimates

We assume that the _true_ relationship between $X$ and $Y$ takes the 
form $Y=f(X)+\epsilon$ for some unknown function $f$, where $\epsilon$
is a mean-zero random error term. If $f$ is to be apporximated by 
a linear function, then we can write this relationship as 

$$Y=\beta_0+\beta_1X+\epsilon$$

Here $\beta_0$ is the intercept term (the value of $Y$ when $X=0$) and
$\beta_1$ is the slope (average increase if $Y$ when $X$ increases by
one-unit). The error term is a catch-all for what we miss with this
simple model: the true relationship is probably not linear, there
may be other variables that cause variation in $Y$, and there may be 
measurement error. We typically assume that the error term is
independent of $X$.

This model defines the _population regression line_, which is the
best linear approximation to the true relationship between $X$ and $Y$.
The least squares regression coefficient estimates the _least squares_
_line_. The true relationship is generally not known for real data,
but the least squares line can always be comptuted using the coefficient
estimates. In real applications, we have access to a set of 
observations from which we can compute the least squares line,
however the population regression line is unobserved.

At first glance, the difference between the population regression
line and the least squares line may seem subtle and confusing.
We only have one data set, and so what does it mean that two
different lines describe the relationship between the predictor
and the response ? 

Fundamentally, the concept of these two lines is a natural extension 
of the standard statistical approach using information from a sample 
to estimate characteristics of a large population. For example, 
suppose that we are interested in knowing the population mean $\mu$
of some random variable $Y$. Unfortunately, $\mu$ is unknown, but we
do have access to $n$ observations from $Y$, which we can write as
$y_1,\ldots,y_n$, and which we can use to estimate $\mu$. A reasonable
estimate is $\hat{\mu}=\bar{y}$, where $\bar{y}=\frac{1}{n}\sum_{i=1}^{n}y_i$ is the sample mean. The sample
mean and the population mean are different, but in general the sample
mean will provide a good estimate of the population mean. In the same
way, the unknown coefficients $\beta_0$ and $\beta_1$ in linear
regression define the population regression line. We seek to estimate
these unknown coefficients using $\hat{\beta}_0$ and $\hat{\beta}_1$.
These coefficient estimates define the least squares line.

The analogy between linear regression and estimation of the mean of a 
random variable is an apt one based on the concept of _bias_. If we use
the sample mean $\hat{\mu}$ to estimate $\mu$, this estimate is
_unbiased_, in the sense that on average, we expect $\hat{\mu}$ to 
equal $\mu$. What exactly does this mean ? It means that on the 
basis of one particular set of observations $y_1,\ldots,y_n$, 
$\hat{\mu}$ might overestimate $\mu$, and on the basis of another set 
of observations, $\hat{\mu}$ might underestimate $\mu$. But if we could
average a huge number of estimates of $\mu$ obtained from a huge number
of sets of observations, then this average would _exactly_ equal $\mu$.
Hence, an unbiased estimator does not _systematically_ over- or under-
estimate the true parameter. The property of unbiasedness holds for the
least squares coefficient estimates as well: if we estimate $\beta_0$
and $\beta_1$ on the basis of a particular data set, then our
estimates won't be exactly equal to $\beta_0$ and $\beta_1$. But 
if we could average the estimates obtained over a huge number of
data sets, then the average of these estimates would be spot on!

We continue the analogy with the estimation of the population mean
$\mu$ of a random variable $Y$. A natural question is as follows:
how accurate is the sample mean $\hat{\mu}$ as an estimate of $\mu$?
We have established that the average of $\hat{\mu}$'s over many data
sets will be very close to $\mu$ but that a single estimate $\hat{\mu}$
may be a substantial understimate or overestimate of $\mu$. How har 
off will that sigle estimate of $\hat{\mu}$ be ? In general, we answer
this question by computing the _standard error_ of $\hat{\mu}$, written
as SE($\hat{\mu}$).

$$Var(\hat{\mu})=SE(\hat{\mu})^2=\frac{\sigma^2}{n}$$

where $\sigma$ is the standard deviation of each of the realization
$y_i$ of $Y$. Roughly speaking, the standard error tells us the average
amount that this estimate $\hat{\mu}$ differs from the actual value
of $\mu$. The equation also tells us how this deviation shrinks 
with $n$, the more observations we have, the smalle the standard
error of $\hat{\mu}$. In a similar vein, we can wonder how close
$\hat{\beta}_0$ and $\hat{\beta}_1$ are to the true values $\beta_0$ and
$\beta_1$. To compute the standard errors associated with $\hat{\beta}_0$
and $\hat{\beta}_1$, we use the following formulas:

$$SE(\hat{\beta}_0)^2=\sigma^2[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^{n}(x_i-\bar{x})^2}]$$, $$SE(\hat{\beta}_1)^2=\frac{\sigma^2}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$

where $\sigma^2=Var(\epsilon)$. For these formulas to be strictly valid,
we need to assume that the errors $\epsilon_i$ for each observation 
are uncorrelated with commom variance $\sigma^2$. This turns out to
be a good approximation. Notice in the formula that $SE(\hat{\beta}_1)$
is smaller when the $x_i$ are more spread out; intuitively we have
more _levarage_ to estimate a slope when this is the case. We also see
that $SE(\hat{\beta}_0)$ would be the same as $SE(\hat{\mu})$ if 
$\bar{x}$ were zero. 

In general, $\sigma^2$ is not known, but can
be estimated from the data. The estimate of $\sigma$ is known as the
_residual standard error_, and is given by the formula 

$$RSE=\sqrt{\frac{RSS}{n-2}}$$

Strictly speaking, when $\sigma^2$ is estimated from the data we
should write $\hat{SE}(\hat{\beta}_1)$ to indicate that an estimate
has been made, but for simplicity of notation we will drop this 
extra "hat".

Standard erros can be used to compute _confidence intervals_. A 95%
confidence interval is defined as a range of values such that with
95% probability, the range will contain the true unknown value 
of the parameter. The range is defined in terms of lower and upper
limits computed from the sample of data. For linear regression,
the 95% confidence interval for $\beta_1$ approximately takes the
form

$$\hat{\beta}_1\pm 2\cdot SE (\hat{\beta}_1)$$

That is, there is approximately a 95% chance that the interval

$$[\hat{\beta}_1-2\cdot SE (\hat{\beta}_1),\hat{\beta}_1+2\cdot SE (\hat{\beta}_1)]$$

will contain the true value of $\beta_1$. Similarly, a confidence
interval for $\beta_0$ approximately takes the form

$$\hat{\beta}_0\pm 2\cdot SE (\hat{\beta}_0)$$

In the case of the advertising data, the 95% confidence interval for
$\beta_0$ is [6.130,7.935] and the 95% confidence interval for 
$\beta_1$ is [0.042,0.053]. Therefore, we can conclude that
in the absence of any advertising, sales will, on average, fall 
somewhere between 6130 and 7940 units. Furthermore, for each
$1000 increase in television and advertising, there will be an
average increase in sales of between 42 and 53 units.

Standard errors can also be used to perform _hypothesis tests_
on the coefficients. The most common hypothesis test involves testing
the _null hypothesis_ of

> $H_0$: There is no relationship between $X$ and $Y$

versus the _alternative hypothesis_

> $H_a$: There is some relationship between $X$ and $Y$

Mathematically, this corresponds to testing

$$H_0:\beta_1=0$$

versus

$$H_a:\beta_1\neq0$$

since if $\beta_1=0$ then the model reduces to $Y=\beta_0+\epsilon$,
and $X$ is not associated with $Y$. To test the null hypothesis, we
need to determine whether $\hat{\beta}_1$, our estimate for $\beta_1$,
is sufficiently far from zero that we can be confident that $\beta_1$
is non-zero. How far is far enough? This of course depends on the
accuracy of $\hat{\beta}_1$. If $SE(\hat{\beta}_1) is small, then
even relatively small values of $\hat{\beta}_1$ may provide strong
evidence that $\beta_1\neq0$, and hence that there is a relationship
between $X$ and $Y$. In contrast, if $SE(\hat{\beta}_1)$ is large, 
then $\hat{\beta}_1$ must be large in absolute value in order for
us to reject the null hypothesis. In practice, we compute a 
_t-statistic_, given by

$$t=\frac{\hat{\beta}_1-0}{SE(\hat{\beta}_1)}$$

which measures the number of standard deviations the number of 
standard deviations that $\hat{\beta}_1$ is away from 0. If
there really is no relationship between $X$ and $Y$, then we
expect that we'll have a _t_-distribution with $n-2$ degrees
of freedom. The t-distribution has a bell shape and for values
of $n$ greater than approximately 30 it is quite similar to
the normal distribution. Consequently, it is a simple matter
to compute the probability of observing any value equal to $|t|$
or larger, assuming $\beta_1=0$. We call this probability the _p-value_.
Roughly speaking, we interpret the p-value as follows: a small
p-value indicates that it is unlikely to observe such a substantial
association between the predictor and the response due to chance,
in the absence of any real association between the predictor
and the response. Hence if we see a small p-value, then we can
infer that there is an association between the predictor and the
response. We _reject the null hypothesis_, that is, we declare
a relationship to exist between $X$ and $Y$, if the p-value is
small enough. Typical p-value cutoffs for rejecting the null
hypothesis are 5 or 1%. When $n=30$, these correspond to t-statistics
of around 2 and 2.75, respectively.

```
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 7.032594   0.457843   15.36   <2e-16 ***
## TV          0.047537   0.002691   17.67   <2e-16 ***
```

The table provides details of the least squares model for the
regression of number of units sold on TV advertising budget
for the *`Advertising`* data. Notice that the coefficients for
$\hat{\beta}_0$ and $\hat{\beta}_1$ are very large relative
to their standard errors, so the t-statistics are also large;
the probabilities of seeing such values if $H_0$ is true are
virtually zero. Hence we can conclude that $\beta_0\neq0$ and
$\beta_1\neq0$.

#### Assessing the Accuracy of the Model

Once we have rejected the null hypothesis in favor of the 
alternative hypothesis, it is natural to want to quantify
_the extent to which the model fits the data_. The quality
of a linear regression fit is typically assessed using two
related quantities: the _residual standard error_ (RSE) and
the $R^2$ statistic.

```
## Residual standard error: 3.259 on 198 degrees of freedom
## Multiple R-squared:  0.6119, Adjusted R-squared:  0.6099 
## F-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16
```

The table displays the RSE, the $R^2$ statistic and the 
F-statistic for the linear regression of number of units
sold on TV advertisign budget.

**Residual Standard Error**

Recall that associated with each observation is an error
term $\epsilon$. Due to the presence of these error terms,
even if we knew the true regression line, we would not be
able to perfectly predict $Y$ from $X$. The RSE is an estimate
of the standard deviation of $\epsilon$. Roughly speaking,
it is the average amount that the response will deviate from
the true regression line. It is computed using the formula

$$RSE=\sqrt{\frac{1}{n-2}RSS}=\sqrt{\frac{1}{n-2}\sum_{i=1}^{n}(y_i-\hat{y}_i)^2}$$

Note that the RSS is given by the formula:

$$RSS=\sum_{i=1}^{n}(y_i-\hat{y}_i)^2$$

In the case of the advertisig data, we see from the linear 
regression output that the RSE is 3.26. In other words, actual 
sales in each market deviate from the true regression by
approximately 3260 units, on average. Another way to think
about this is that even if the model were correct and the
true values of the unknown coefficients $\beta_0$ and $\beta_1$
were known exactly, any prediction of sales on the basis of
TV advertising would still be off by about 3260 units on average.
Of course, wheter or not 3260 units is an acceptable prediction
error depends on the problem context. In the advertising data
set, the mean volume of *`sales`* over all markets is approximately
14000 units, and so the percentage error is 3260/14000=23%.

The RSE is considered a measure of the _lack of fit_ of the model
to the data. If the predictions obtained using the model are
very close to the true outcome values, $\hat{y}_i\approx y_i$ for
$i=1,\ldots,n$, then RSE will be small and we can conclude that the 
model fits the data very well. On the other hand, if $\hat{y}_i$, 
is very far from $y_i$, for one or more observations, then the
RSE may be quite large, indicating that the model doesn't fit
the data well.

**$R^2$ Statistic**

The RSE provides an absolute measure of lack of fit of the model
to the data. But since it is measured in the units of $Y$, it is
not always clear what constitutes a good RSE. The $R^2$ statistic
provides an alternative measure of fit. It takes the form of 
a _proportion_, the proportion of variance explained, and so it
always takes on a value between 0 and 1, and is independent of
the scale of Y.

To calculate $R^2$, we use the formula

$$R^2=\frac{TSS-RSS}{TSS}=1-\frac{RSS}{TSS}$$

where $TSS=\sum(y_i-\bar{y})^2$ is the _total sum of squares_, and
$RSS=\sum_{i=1}^{n}(y_i-\hat{y}_i)^2$. TSS measures the total 
variance in the response $Y$, and can be thought of as the amount
of variability inherent in the response before the regressino
is performed. In contrast, RSS measures the amount of variability
that is left unexplained after performing the regression. Hence,
$TSS-RSS$ measures the amount of variability in the response that 
is explained by performing the regression, and $R^2$ measures the
_proportion of variability in Y that can be explained using X_.
An $R^2$ statistic that is close to 1 indicates that a large
proportion of the variability in the response has been explained 
by the regression. A number near 0 indicates that the regression
did not explain much of the variability in the response; this
might occur because the linear model is wrong, or the inherent
error $\sigma^2$ is high, or both. In the previous table, the 
$R^2$ was 0.61, and so just under two-thirds of the variability 
in *`sales`* is explained by a linear regression on *`TV`*.

The $R^2$ statistic has an interpretational advantage over the RSE,
since unlike the RSE, it always lies between 0 and 1. However,
it can still be challenging to determine what is _good $R^2$_ value,
and in general, this will depend on the application. For instance,
in certain problems in physics, we may know that the data truly
comes from a linear model with a small residual error. In this
case, we would expect to see an $R^2$ value that is extremely
close to 1, and a substantially smaller $R^2$ value might indicate
a serious problem with the experiment in which the data were
generated. On the other hand, in typical applications in biology,
psychology, marketing, and other domains, the linear model is at
best an extremely rough approximation to the data and residual 
errors due to other unmeasured factors are often very large.
In this setting, we would expect only a very small proportion of
the variance in the response to be explained by the predictor,
and an $R^2$ value well below 0.1 might be more realistic!

The $R^2$ statistic is a measure of the linear relationship between
$X$ and $Y$. Recall that _correlation_, defined as

$$Cor(X,Y)=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i-\bar{y})^2}}$$

is also a measure of the linear relationship between $X$ and $Y$. 
This suggests that we might be able to use $r=Cor(X,Y)$ instead
of $R^2$ in order to asses the fit of the linear model. In fact,
it can be shown that in the simple linear regression setting,
$R^2=r^2$. In other words, the squared correlation and the $R^2$ 
statistic are identical.

### Multiple Linear Regression

Simple linear regression is a useful approach for predicting a 
response on the basis of a sigle predictor variable. However, in
practice we often have more than one predictor. For example, in 
the *`Advertising`* data, we have examined the relationship
between sales and TV advertising. We also have data for the
amount of money spent advertising on the radio and in newspapers,
and we may want to know whether either of these two media is
associated with sales. How can we extend our analysis of the
advertising data in order to accomodate these two additional 
predictors?

One option is to run three separate simple linear regressions,
each of which uses a different advertising medium as a predictor.
For instance, we can fit a simple linear regression to predict
sales on the basis of the amount spent on radio advertisements.

```
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  9.31164    0.56290  16.542   <2e-16 ***
## Radio        0.20250    0.02041   9.921   <2e-16 ***
##
##
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 12.35141    0.62142   19.88  < 2e-16 ***
## Newspaper    0.05469    0.01658    3.30  0.00115 ** 
```

We find that a $1000 increase in spending on radio advertising
is associated with an increase in sales by around 203 units.
A $1000 increase in newspaper advertising budget is associated with
an increase in sales by approximately 55 units.

However, the approach of fitting a separate simple linear
regression model for each predictor is not entirely satisfactory.
First of all, it is unclear how to make a single prediction
of sales given levels of the three advertising media budgets,
since each of the budgets is associated with a separate regression
equation. Second, each of the three regression equations ignores
the other two media in forming estimates for the regression
coefficients. We will see that if the media budgets are correlated
with each other in the 200 markets that constitute our data set,
then this can lead to very misleading estimates of the individual
media effects on sales.

Instead of fitting a separate simple linear regression model
for each predictor, a better approach is to extend the simple
linear regression model so that it can directly accomodate
multiple predictors. We can do this by giving each predictors
a separate slope coefficient in a single model. In geeneral,
suppose that we have $p$ distinct predictors. Then the multiple
linear regression model thakes the form

$$Y=\beta_0+\beta_1 X_1+\beta_2 X_2+\cdots+\beta_p X_p+\epsilon$$

where $X_j$ represent the $j$ th predictor and $\beta_j$ quantifies
the association between that variable and the response. We interpret
$\beta_j$ as the _average_ effect on $Y$ of a one unit increase 
in $X_j$, _holding all other predictors fixed_. In the advertising
example, becames

$$sales=\beta_0+\beta_1\times TV+\beta_2\times radio+\beta_3\times newspaper + \epsilon$$

#### Estimating the Regression Coefficients



### Simple Linear Regression

Here we load the `MASS` package, which is a very large collection of
data sets and functions. We also load the ISLR package, which includes
the data sets assoctiated with the book.

```{r}
library(MASS)
library(ISLR)
```

The `MASS` library contains the `Boston` data set, which records
`medv` (median house value) for 506 neighborhoods around Boston.
We will seek to predict `medv` using 13 predictors such as `rm` 
(average number of rooms per house), `age` (average age of houses), 
and `lstat` (percent of households with low socioeconomic status).

```{r}
names(Boston)
```

We will start by using the `lm()` function to fit a simple
linear regression model, with `medv` as the response and 
`lstat` as the predictor.

```{r}
lm.fit=lm(medv~lstat, data=Boston)
lm.fit
```

Some basic information about the model is output. For more
detailed information, we use `summary(lm.fit)`. This gives us
p-values and standard errors for the coefficients, as well as
the R-squared statistic and F-statistic for the model.

```{r}
summary(lm.fit)
```

We can use `names()` function in order to find out what other
pieces of information are stored in `lm.fit`. Although we can
extract these quantities by name, `lm.fit$coefficients`, it is
safer to use the extractor function like `coef()` to access them.

```{r}
names(lm.fit)
lm.fit$coefficients
coef(lm.fit)
```

In order to obtain a confidence interval for the coefficient
estimates, we can use the `confint()` command.

```{r}
confint(lm.fit)
```

The `predict()` function can be used to produce confidence
intervals and prediction intervals for the prediction of `medv`
for a given value of `lstat`.

```{r}
predict(lm.fit, data.frame(lstat=c(5,10,15)),
        interval="confidence")
predict(lm.fit, data.frame(lstat=c(5,10,15)),
        interval="prediction")
```

The 95% confidence interval associated with `lstat` value of
10 is (24.47, 25.63), and the 95% prediction interval is
(12.828, 37.28). As expected, the confidence and prediction
intervals are centered around the same point (25.05 for `medv`
when `lstat` equals 10), but the latter are substantially wider.

We will now plot `medv` and `lstat` along with the least squares
regression line using the `plot()` and `abline()` functions.

The `abline()` function can be used to draw any line, not just
the least squares regression line. To draw a line with intercept
`a` and slope `b`, we type `abline(a,b)`. The `lwd=3()` command 
causes the width of the regression line to be increased by a 
factor of 3; this works for the `plot()` and `lines()`  function
also. We can use the `pch` option to create different plotting
symbols.

```{r}
plot(Boston$lstat,Boston$medv)
abline(lm.fit, lwd=3,col="red")
```

Four diagnostic plots are automatically produced by
applying the `plot()` function directly to the output from `lm()`.
It is often convenient to view all four plots together. We can
achieve this by using the `par()` function, which tell `R` to
split the display screen into separete panels so that multiple 
plots can be viewed simultaneously.

```{r}
par(mfrow=c(2,2))
plot(lm.fit)
```

Alternatively, we can compute the residuals from a linear 
regression fit using the `residuals()` function. The function
`rstudent()` will return the studentized residuals, and we 
can use this function to plot the redisuals against the
fitted values.

```{r}
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
```

On the basis of the residuals plots, there is some evidence
of non-linearity. Leverage statistics can be computed for any
number of predictors using the `hatvalues()` function.

The `which.max()` function identifies the index of the largest
element of a vector. In this case, it tells us which obs has
the larsgest leverage

```{r}
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```

### Multiple Linear Regression

In order to fit a multiple linear regression model using 
least squres, we again use the `lm()` function.

```{r}
lm.fit=lm(medv~lstat+age, data=Boston)
summary(lm.fit)
```

The `Boston` data set contains 13 variables, and so it would be
cumbersome to have to tye all of these in order to perform a
regression using all of the predictors. Instead, we can use the
following short-hand:

```{r}
lm.fit=lm(medv~., data=Boston)
summary(lm.fit)
```

We can access the individual components of a summary object by
name. 

```{r}
summary(lm.fit)$r.sq # give us the R-squared
summary(lm.fit)$sigma # give us the RSE
```

The `vif()` function, part of the `car` package, can be used to
compute variance inflation factors. Most VIF's are low to moderate
for this data.

```{r}
library(car)
vif(lm.fit)
```

What if we would like to perform a regression using all of the
variable but one?

```{r}
lm.fit1=lm(medv~.-age,data=Boston)
summary(lm.fit1)
```

### Interaction Terms

It is easy to include interaction terms in a linear model
using the `lm()` function. The syntax `lstat:black` tells `R`
to include an interaction term between `lstat` and `black`.
The syntax `lstat*age` simultaneously includes `lstat`, `age`,
and the interaction term `lstat`x`age` as predictors; it is
a shorthand for `lstat+age+lstat:age`.

```{r}
summary(lm(medv~lstat*age, data=Boston))
```

### Non-linear Transformations of the Predictors

The `lm()` function can also accomodate non-linear transformations
of the predictors. For instance, given a predictor X, we can
create a predictor X-squared using `I(X^2)`. The function `I()`
is needed since the `^` has a special meaning in a formula;
wrapping as we do allows the standard usage in `R`, which
is to raise `X` to the power `2`. We now perform a regression
of `medv` onto `lstat` and `lstat`-squared.

```{r}
lm.fit2=lm(medv~lstat+I(lstat^2),data=Boston)
summary(lm.fit2)
```

The near-zero p-value associated with the quadratic term suggests
that it leads to an improved model. We use the `anova()` function
to further quantify the extent to which the quadratitc fit is
superior to the linear fit.

```{r}
lm.fit=lm(medv~lstat, data=Boston)
anova(lm.fit,lm.fit2)
```

Here Model 1 represents the linear submodel containing only one
predictor, `lstat`, while Model 2 corresponds to the larger
quadratic model that has two predictors, `lstat` and `last`-squared.
The `anova()` function performs a hypothesis test comparing the 
two models. The null hypothesis is that the two models fit the
data equally well, and the alternative hypothesis is that the
full model is superior. Here the F-statistic is 135 and the
associated p-value is virtually zero. This provides very clear
evidence that the model containing the predictors `lstat` and
`last'-squared is far superior to the model that only contains
the predictor `lstat`. This is not surprising, since earlier we
saw evidence for non-linearity in the relationship between
`medv` and `lstat`. If we type

```{r}
par(mfrow=c(2,2))
plot(lm.fit2)
```

then we see that when the `lstat`-squared term is included in
the model, there is little discernible pattern in the residuals.

In order to create a cubic fit, we can include a predictor of the
form `I(X^3)`. However, this approach can start to get cumbersome
for highr-order polynomials. A better approach involves using
the `poly()` function to create the polynomial within `lm()`.
For example the following command produces a fifth-order
polynomial fit.

```{r}
lm.fit5=lm(medv~poly(lstat,5),data=Boston)
summary(lm.fit5)
```

This suggests that including additional polynomial terms, up to
fifth order, leads to an improvement in the model fit. However,
further investigation of the data reveals that no polynomials
terms beyond fifth order have significant p-values in regression
fit.

Of course, we are in no way restricted to using polynomial 
transformations of the predictors. Here we try a log transformation

```{r}
summary(lm(medv~log(rm),data=Boston))
```

### Qualitative Predictors 

We will now examine the `Carseats` data, which is part of the
`ISLR` library. We will attempt to predict `Sales` in 400
locations based on a number of predictors.

```{r}
names(Carseats)
```

The `Carseats` data includes qualitative predictors such as
`Shelveloc`, an indicator of the quality of the shelving location,
the space within a store in which the car seat is displayed, at
each location. The predictor `Shelveloc` takes on three possible
values, _Bad_, _Medium_ and _Good_.

Given a qualitative variable such as `Shelveloc`, `R` generates
dummy variables automatically. Below we fit a multiple regression
 model that includes some interaction termis.
 
 
```{r}
lm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)
```

The `contrast()` function returns the coding that `R` uses
for the dummy variables.

```{r}
contrasts(Carseats$ShelveLoc)
```

`R` has created a `ShelveLocGood` dummy variable that takes on a
value of 1 if the shelving location is good, and 0 otherwise.
It has also created a `ShelveLocMedium` dummy variable that 
equals 1 if the shelving location is medium, and 0 otherwise.
A bad shelving location corresponds to a zero for each of the
two dummy variables. The fact that the coefficient for 
`ShelveLocGood`  in the regression output is positive indicates
that a good shelvig location is associated with high sales.
And `ShelveLocMedium` has a smaller positive coefficient,
indicating that a medium shelving location leads to higher
sales than a bad shelving location but lower sales than a good
shelving location.

### Writing Functions

As we have seen, `R` comes with useful functions, and still more
functions are available by way of `R` libraries. However, we
will often be interested in performing an operation for which
no function is available. In this setting, we may want to write
our own function. For instance, below we provide a simple function
that reads in the `ISLR` and `MASS` libraries, called 
`LoadLibraries()`. 
The `{` symbol informs `R` that multiple commands are about to
be input.

```{r}
LoadLibraries=function(){
  library(ISLR)
  library(MASS)
  print("The libraries have been loaded.")
}
LoadLibraries # tell us what is in the function
LoadLibraries() # we call the  function
```

